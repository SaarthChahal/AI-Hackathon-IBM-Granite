{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# AI Service Deployment Notebook\nThis notebook contains steps and code to test, promote, and deploy an AI Service\ncapturing logic to implement RAG pattern for grounded chats.\n\n**Note:** Notebook code generated using Prompt Lab will execute successfully.\nIf code is modified or reordered, there is no guarantee it will successfully execute.\nFor details, see: <a href=\"/docs/content/wsj/analyze-data/fm-prompt-save.html?context=wx\" target=\"_blank\">Saving your work in Prompt Lab as a notebook.</a>\n\n\nSome familiarity with Python is helpful. This notebook uses Python 3.11.\n\n## Contents\nThis notebook contains the following parts:\n\n1. Setup\n2. Initialize all the variables needed by the AI Service\n3. Define the AI service function\n4. Deploy an AI Service\n5. Test the deployed AI Service\n\n## 1. Set up the environment\n\nBefore you can run this notebook, you must perform the following setup tasks:"}, {"metadata": {}, "cell_type": "markdown", "source": "### Connection to WML\nThis cell defines the credentials required to work with watsonx API for both the execution in the project, \nas well as the deployment and runtime execution of the function.\n\n**Action:** Provide the IBM Cloud personal API key. For details, see\n<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n"}, {"metadata": {}, "cell_type": "code", "source": "import os\nimport getpass\nimport requests\n\ndef get_credentials():\n    return {\n        \"url\" : \"https://us-south.ml.cloud.ibm.com\",\n        \"apikey\" : getpass.getpass(\"Please enter your api key (hit enter): \")\n    }\n\ndef get_bearer_token():\n    url = \"https://iam.cloud.ibm.com/identity/token\"\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    data = f\"grant_type=urn:ibm:params:oauth:grant-type:apikey&apikey={credentials['apikey']}\"\n\n    response = requests.post(url, headers=headers, data=data)\n    return response.json().get(\"access_token\")\n\ncredentials = get_credentials()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watsonx_ai import APIClient\n\nclient = APIClient(credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Connecting to a space\nA space will be be used to host the promoted AI Service.\n"}, {"metadata": {}, "cell_type": "code", "source": "space_id = \"044ec471-2604-4d4a-80c2-1e0c4a1d6105\"\nclient.set.default_space(space_id)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Promote asset(s) to space\nWe will now promote assets we will need to stage in the space so that we can access their data from the AI service.\n"}, {"metadata": {}, "cell_type": "code", "source": "source_project_id = \"c07e23d7-13de-4702-96ff-f1f5b5a2457d\"\nvector_index_id = client.spaces.promote(\"6b62d768-fb69-4833-acbf-daa84178f8ba\", source_project_id, space_id)\nprint(vector_index_id)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2. Create the AI service function\nWe first need to define the AI service function\n\n### 2.1 Define the function"}, {"metadata": {}, "cell_type": "code", "source": "params = {\n    \"space_id\": space_id, \n    \"vector_index_id\": vector_index_id\n}\n\ndef gen_ai_service(context, params = params, **custom):\n    # import dependencies\n    import json\n    from ibm_watsonx_ai.foundation_models import ModelInference\n    from ibm_watsonx_ai import APIClient\n    import gzip\n    import chromadb\n    import random\n    import string\n    from ibm_watsonx_ai.foundation_models.embeddings.sentence_transformer_embeddings import SentenceTransformerEmbeddings\n\n    vector_index_id = params.get(\"vector_index_id\")\n\n    # Get credentials token\n    credentials = {\n        \"url\": \"https://us-south.ml.cloud.ibm.com\",\n        \"token\": context.generate_token()\n    }\n\n    # Setup client\n    client = APIClient(credentials)\n    space_id = params.get(\"space_id\")\n    client.set.default_space(space_id)\n\n    # Get vector index details\n    vector_index_details = client.data_assets.get_details(vector_index_id)\n    vector_index_properties = vector_index_details[\"entity\"][\"vector_index\"]\n    top_n = 20 if vector_index_properties[\"settings\"].get(\"rerank\") else int(vector_index_properties[\"settings\"][\"top_k\"])\n\n    def rerank( client, documents, query, top_n ):\n        from ibm_watsonx_ai.foundation_models import Rerank\n\n        reranker = Rerank(\n            model_id=\"cross-encoder/ms-marco-minilm-l-12-v2\",\n            api_client=client,\n            params={\n                \"return_options\": {\n                    \"top_n\": top_n\n                },\n                \"truncate_input_tokens\": 512\n            }\n        )\n\n        reranked_results = reranker.generate(query=query, inputs=documents)[\"results\"]\n\n        new_documents = []\n        \n        for result in reranked_results:\n            result_index = result[\"index\"]\n            new_documents.append(documents[result_index])\n            \n        return new_documents\n    emb = SentenceTransformerEmbeddings('sentence-transformers/all-MiniLM-L6-v2')\n\n\n    def hydrate_chromadb():\n        data = client.data_assets.get_content(vector_index_id)\n        content = gzip.decompress(data)\n        stringified_vectors = str(content, \"utf-8\")\n        vectors = json.loads(stringified_vectors)\n        \n        chroma_client = chromadb.Client()\n        \n        # make sure collection is empty if it already existed\n        collection_name = \"my_collection\"\n        try:\n            collection = chroma_client.delete_collection(name=collection_name)\n        except:\n            print(\"Collection didn't exist - nothing to do.\")\n        collection = chroma_client.create_collection(name=collection_name)\n        \n        vector_embeddings = []\n        vector_documents = []\n        vector_metadatas = []\n        vector_ids = []\n        \n        for vector in vectors:\n            vector_embeddings.append(vector[\"embedding\"])\n            vector_documents.append(vector[\"content\"])\n            metadata = vector[\"metadata\"]\n            lines = metadata[\"loc\"][\"lines\"]\n            clean_metadata = {}\n            clean_metadata[\"asset_id\"] = metadata[\"asset_id\"]\n            clean_metadata[\"asset_name\"] = metadata[\"asset_name\"]\n            clean_metadata[\"url\"] = metadata[\"url\"]\n            clean_metadata[\"from\"] = lines[\"from\"]\n            clean_metadata[\"to\"] = lines[\"to\"]\n            vector_metadatas.append(clean_metadata)\n            asset_id = vector[\"metadata\"][\"asset_id\"]\n            random_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10))\n            id = \"{}:{}-{}-{}\".format(asset_id, lines[\"from\"], lines[\"to\"], random_string)\n            vector_ids.append(id)\n\n        collection.add(\n            embeddings=vector_embeddings,\n            documents=vector_documents,\n            metadatas=vector_metadatas,\n            ids=vector_ids\n        )\n        return collection\n    \n    chroma_collection = hydrate_chromadb()\n\n    def proximity_search( question, emb ):\n        query_vectors = emb.embed_query(question)\n        query_result = chroma_collection.query(\n            query_embeddings=query_vectors,\n            n_results=top_n,\n            include=[\"documents\", \"metadatas\", \"distances\"]\n        )\n        \n        documents = list(reversed(query_result[\"documents\"][0]))\n        metadatas = reversed(query_result[\"metadatas\"][0])\n        distances = reversed(query_result[\"distances\"][0])\n        results = []\n        for metadata, distance in zip(metadatas, distances):\n            results.append({\n                \"metadata\": metadata,\n                \"score\": distance\n            })\n\n        return {\n            \"results\": results,\n            \"documents\": documents\n        }\n\n    # Functions used for inferencing\n    def format_input(messages, documents):\n        context = \"\\n\".join(documents)\n        system = f\"\"\"<|start_of_role|>system<|end_of_role|>You are Granite, an AI language model developed by IBM in 2024. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. You are a AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, i.e., ensure that your response is correct given the context and user query, and that it is grounded in the context. Furthermore, make sure that the response is supported by the given document or context. Always make sure that your response is relevant to the question. If an explanation is needed, first provide the explanation or reasoning, and then give the final answer. Avoid repeating information unless asked.<|end_of_text|>\n\"\"\"\n        messages_section = []\n\n        for index,value in enumerate(messages, start=0):\n            content = value[\"content\"]\n            user_template = f\"\"\"<|start_of_role|>user<|end_of_role|>{content}<|end_of_text|>\n\"\"\"\n            assistant_template = f\"\"\"<|start_of_role|>assistant<|end_of_role|>{content}<|end_of_text|>\n\"\"\"\n            grounded_user_template = f\"\"\"<|start_of_role|>user<|end_of_role|>Use the following pieces of context to answer the question.\n\n{context}\n\nQuestion: {content}<|end_of_text|>\n\"\"\"\n\n            formatted_entry = user_template if value[\"role\"] == \"user\" else assistant_template\n            if (index == len(messages)-1):\n                formatted_entry = grounded_user_template\n            \n            messages_section.append(formatted_entry)\n\n        messages_section = \"\".join(messages_section)\n        prompt = f\"\"\"{system}{messages_section}<|start_of_role|>assistant<|end_of_role|>\"\"\"\n        return prompt\n    \n    def inference_model( messages, documents, access_token, stream ):\n        prompt = format_input(messages, documents)\n        model_id = \"ibm/granite-3-8b-instruct\"\n        parameters =  {\n            \"decoding_method\": \"greedy\",\n            \"max_new_tokens\": 900,\n            \"min_new_tokens\": 0,\n            \"repetition_penalty\": 1\n        }\n        inference_credentials = {\n            \"url\": credentials.get(\"url\"),\n            \"token\": access_token\n        }\n        model = ModelInference(\n            model_id = model_id,\n            params = parameters,\n            credentials = inference_credentials,\n            space_id = space_id\n        )\n        # Generate grounded response\n        if (stream == True):\n            generated_response = model.generate_text_stream(prompt=prompt, guardrails=False)\n        else:\n            generated_response = model.generate_text(prompt=prompt, guardrails=False)\n\n        return generated_response\n\n    def get_internal_client(access_token):\n        # Setup client\n        internal_credentials = {\n            \"url\": credentials.get(\"url\"),\n            \"token\": access_token\n        }\n        internal_client = APIClient(internal_credentials)\n        space_id = params.get(\"space_id\")\n        internal_client.set.default_space(space_id)\n        return internal_client\n\n    def generate(context):\n        payload = context.get_json()\n        messages = payload.get(\"messages\")\n        access_token = context.get_token()\n        last_question = messages[-1].get(\"content\")\n \n        internal_client = get_internal_client(access_token)\n\n\n        # Proximity search\n        documents = proximity_search(last_question, emb)[\"documents\"]\n\n        if vector_index_properties[\"settings\"].get(\"rerank\"):\n            documents = rerank(internal_client, documents, last_question, vector_index_properties[\"settings\"][\"top_k\"])\n    \n        \n        # Grounded inferencing\n        generated_response = inference_model(messages, documents, access_token, False)\n        \n        execute_response = {\n            \"headers\": {\n                \"Content-Type\": \"application/json\"\n            },\n            \"body\": {\n                \"choices\": [{\n                    \"index\": 0,\n                    \"message\": {\n                    \"role\": \"assistant\",\n                    \"content\": generated_response\n                    }\n                }]\n            }\n        }\n\n        return execute_response\n\n    def generate_stream(context):\n        payload = context.get_json()\n        messages = payload.get(\"messages\")\n        access_token = context.get_token()\n        last_question = messages[-1].get(\"content\")\n\n        internal_client = get_internal_client(access_token)\n\n\n        # Proximity search\n        documents = proximity_search(last_question, emb)[\"documents\"]\n\n        if vector_index_properties[\"settings\"].get(\"rerank\"):\n            documents = rerank(internal_client, documents, last_question, vector_index_properties[\"settings\"][\"top_k\"])\n    \n        \n        # Grounded inferencing\n        response_stream = inference_model(messages, documents, access_token, True)\n\n        for chunk in response_stream:\n            chunk_response = {\n                \"choices\": [{\n                    \"index\": 0,\n                    \"delta\": {\n                        \"role\": \"assistant\",\n                        \"content\": chunk\n                    }\n                    \n                }]\n            }\n            yield chunk_response\n\n    return generate, generate_stream\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2 Test locally"}, {"metadata": {}, "cell_type": "code", "source": "# Initialize AI Service function locally\nfrom ibm_watsonx_ai.deployments import RuntimeContext\n\ncontext = RuntimeContext(api_client=client)\n\nstreaming = False\nfindex = 1 if streaming else 0\nlocal_function = gen_ai_service(context, vector_index_id=vector_index_id, space_id=space_id)[findex]\nmessages = []", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "local_question = \"Change this question to test your function\"\n\nmessages.append({ \"role\" : \"user\", \"content\": local_question })\n\ncontext = RuntimeContext(api_client=client, request_payload_json={\"messages\": messages})\n\nresponse = local_function(context)\n\nresult = ''\n\nif (streaming):\n    for chunk in response:\n        print(chunk[\"choices\"][0][\"message\"][\"delta\"][\"content\"], end=\"\", flush=True)\nelse:\n    print(response)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3. Store and deploy the AI Service\nBefore you can deploy the AI Service, you must store the AI service in your watsonx.ai repository."}, {"metadata": {}, "cell_type": "code", "source": "# Look up software specification for the AI service\nsoftware_spec_id_in_project = \"4e852902-d936-42e1-bdba-70472ad5d47e\"\nsoftware_spec_id = \"\"\n\ntry:\n\tsoftware_spec_id = client.software_specifications.get_id_by_name(\"ai-service-v4-software-specification\")\nexcept:\n    software_spec_id = client.spaces.promote(software_spec_id_in_project, source_project_id, space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Define the request and response schemas for the AI service\nrequest_schema = {\n    \"application/json\": {\n        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"messages\": {\n                \"title\": \"The messages for this chat session.\",\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"role\": {\n                            \"title\": \"The role of the message author.\",\n                            \"type\": \"string\",\n                            \"enum\": [\"user\",\"assistant\"]\n                        },\n                        \"content\": {\n                            \"title\": \"The contents of the message.\",\n                            \"type\": \"string\"\n                        }\n                    },\n                    \"required\": [\"role\",\"content\"]\n                }\n            }\n        },\n        \"required\": [\"messages\"]\n    }\n}\n\nresponse_schema = {\n    \"application/json\": {\n        \"oneOf\": [{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"description\":\"AI Service response for /ai_service_stream\",\"properties\":{\"choices\":{\"description\":\"A list of chat completion choices.\",\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"index\":{\"type\":\"integer\",\"title\":\"The index of this result.\"},\"delta\":{\"description\":\"A message result.\",\"type\":\"object\",\"properties\":{\"content\":{\"description\":\"The contents of the message.\",\"type\":\"string\"},\"role\":{\"description\":\"The role of the author of this message.\",\"type\":\"string\"}},\"required\":[\"role\"]}}}}},\"required\":[\"choices\"]},{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"description\":\"AI Service response for /ai_service\",\"properties\":{\"choices\":{\"description\":\"A list of chat completion choices\",\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"index\":{\"type\":\"integer\",\"description\":\"The index of this result.\"},\"message\":{\"description\":\"A message result.\",\"type\":\"object\",\"properties\":{\"role\":{\"description\":\"The role of the author of this message.\",\"type\":\"string\"},\"content\":{\"title\":\"Message content.\",\"type\":\"string\"}},\"required\":[\"role\"]}}}}},\"required\":[\"choices\"]}]\n    }\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Store the AI service in the repository\nai_service_metadata = {\n    client.repository.AIServiceMetaNames.NAME: \"lablab IBM watsonx challenge\",\n    client.repository.AIServiceMetaNames.DESCRIPTION: \"\",\n    client.repository.AIServiceMetaNames.SOFTWARE_SPEC_ID: software_spec_id,\n    client.repository.AIServiceMetaNames.CUSTOM: {},\n    client.repository.AIServiceMetaNames.REQUEST_DOCUMENTATION: request_schema,\n    client.repository.AIServiceMetaNames.RESPONSE_DOCUMENTATION: response_schema,\n    client.repository.AIServiceMetaNames.TAGS: [\"wx-vector-index\"]\n}\n\nai_service_details = client.repository.store_ai_service(meta_props=ai_service_metadata, ai_service=gen_ai_service)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Get the AI Service ID\n\nai_service_id = client.repository.get_ai_service_id(ai_service_details)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Deploy the stored AI Service\ndeployment_custom = {}\ndeployment_metadata = {\n    client.deployments.ConfigurationMetaNames.NAME: \"lablab IBM watsonx challenge\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {},\n    client.deployments.ConfigurationMetaNames.CUSTOM: deployment_custom,\n    client.deployments.ConfigurationMetaNames.DESCRIPTION: \"\",\n    client.repository.AIServiceMetaNames.TAGS: [\"wx-vector-index\"]\n}\n\nfunction_deployment_details = client.deployments.create(ai_service_id, meta_props=deployment_metadata, space_id=space_id)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4. Test AI Service"}, {"metadata": {}, "cell_type": "code", "source": "# Get the ID of the AI Service deployment just created\n\ndeployment_id = client.deployments.get_id(function_deployment_details)\nprint(deployment_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "messages = []\nremote_question = \"Change this question to test your function\"\nmessages.append({ \"role\" : \"user\", \"content\": remote_question })\npayload = { \"messages\": messages }", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "result = client.deployments.run_ai_service(deployment_id, payload)\nif \"error\" in result:\n    print(result[\"error\"])\nelse:\n    print(result)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Next steps\nYou successfully deployed and tested the AI Service! You can now view\nyour deployment and test it as a REST API endpoint.\n\n<a id=\"copyrights\"></a>\n### Copyrights\n\nLicensed Materials - Copyright \u00a9 2024 IBM. This notebook and its source code are released under the terms of the ILAN License.\nUse, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n\n**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for watsonx.ai Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n\nBy downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  "}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}}, "nbformat": 4, "nbformat_minor": 0}